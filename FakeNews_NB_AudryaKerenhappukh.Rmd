---
title: "Naive Bayes Assignment - Fake News Classification"
author: 
- Audrya Kerenhappukh - Author 
- Ryan Kokke - Reviewer 

date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: '2'
    df_print: paged
  html_notebook:
    toc: yes
    toc_depth: 2
---


```{r message=TRUE, warning = TRUE, include = FALSE}
library(tidyverse)
library(dplyr)
library(wordcloud)
library(tm)
library(caret)
library(e1071)
library(readr)
```

## Business Understanding 
Develop a machine learning program to identify when an article might be fake news

## Import Document and Data Understanding 
```{r message=TRUE, warning = TRUE, include = FALSE}

url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv"

rawDF <- read_csv(url) %>% na.omit # Assign to new variable and elimate NaN values


```
Because text could be really long we're trying to reduce the file size by making a new variable called 'author.title' 
```{r}
rawDF$author.title <- paste(rawDF$author, rawDF$title)

cbind(colnames(rawDF))
```

```{r}
# Create a new data set with only two variables: 'label' and 'author.title'
cleanDF <- rawDF[c(5,6)]

```


The labels in the dataset are still numerical, we would want to factor them into "unreliable" and "reliable"

```{r}

cleanDF$label <- factor(cleanDF$label, levels = c(1,0), labels = c("unreliable", "reliable")) %>% relevel("reliable")

```

Visually inspect the data by creating worldclouds
```{r message=TRUE, warning = FALSE, include = TRUE}
unreliable <- cleanDF %>% 
  filter(label == "unreliable")
reliable <- cleanDF %>% 
  filter(label == "reliable")

wordcloud(unreliable$author.title, max.words = 30, scale = c(4, 0.8), colors= c("indianred1","indianred2","indianred3","indianred"))
wordcloud(reliable$author.title, max.words = 30, scale = c(4, 0.8), colors= c("lightsteelblue1","lightsteelblue2","lightsteelblue3","lightsteelblue"))
```


## Preparation 
### Create corpus 

First we need to create a corpus, which refers to a collection of text documents. In our case each  is considered a text document

#### Creating corpus for Training data 

```{r}

rawCorpus <- Corpus(VectorSource(cleanDF$author.title))

```

```{r warning=FALSE}
cleanCorpus <- rawCorpus %>% tm_map(tolower) %>% tm_map(removeNumbers)
cleanCorpus <- cleanCorpus %>% tm_map(tolower) %>% tm_map(removeWords, stopwords()) %>% tm_map(removePunctuation)
```

```{r warning=FALSE}
cleanCorpus <- cleanCorpus %>% tm_map(stripWhitespace)

```
See the difference between Raw vs Clean Corpus 
```{r}
tibble(Raw = rawCorpus$content[1:2], Clean = cleanCorpus$content[1:2])
```

```{r}
cleanDTM <- cleanCorpus %>% DocumentTermMatrix
inspect(cleanDTM)
```
```{r}

```

Before we can start modelling, it is important to split the data sets into train and test sets. We are using 90% of datasets for training, and 10% for test.

```{r}
set.seed(1234)
trainIndex <- createDataPartition(cleanDF$label, p = .9, 
                                  list = FALSE, 
                                  times = 1)
head(trainIndex)

# Apply split indices to DF
trainDF <- cleanDF[trainIndex, ]
testDF <- cleanDF[-trainIndex, ]

# Apply split indices to Corpus
trainCorpus <- cleanCorpus[trainIndex]
testCorpus <- cleanCorpus[-trainIndex]

# Apply split indices to DTM
trainDTM <- cleanDTM[trainIndex, ]
testDTM <- cleanDTM[-trainIndex, ]

```

### Eliminating Words with lower frequencies 

```{r}
freqwords <- cleanDTM %>% findFreqTerms(1000)
trainDTM <- DocumentTermMatrix(trainCorpus, list(dictionary = freqwords))
testDTM <- DocumentTermMatrix(testCorpus, list(dictionary = freqwords))

inspect(trainDTM)
inspect(testDTM)

```

Another issue is that the Naive Bayes classifier is typically trained on categorical features. We now have numerical matrix with word counts. We will transform the counts into a factor that simply indicates whether the word appears in the document or not. Weâ€™ll first build our own function for this and then apply it to each column in the DTM.
```{r}
convert_counts <- function(x) {
  x <- ifelse(x > 0, 1, 0) %>% 
    factor(levels = c(0,1), labels = c("No", "Yes")) 
}

nColsDTM <- dim(trainDTM)[2]
trainDTM <- apply(trainDTM, MARGIN = 2, convert_counts)
testDTM <- apply(testDTM, MARGIN = 2, convert_counts)

head(trainDTM)
```
## Modeling and Evaluation

Create the model 
```{r}
nbayesModel <-  naiveBayes(trainDTM, trainDF$label, laplace = 1)
```

```{r}
predVec <- predict(nbayesModel, testDTM)
```

```{r}
cm <- confusionMatrix(predVec, testDF$label, positive = "unreliable", dnn = c("Prediction", "True"))

cm
```

## Visualization of Confusion Matrix 
```{r Chart Visualization }
table <- as.data.frame(cm$table)

plotTable <- table %>%
  mutate(goodbad = ifelse(table$Prediction == table$True, "good", "bad")) %>%
  group_by(True) %>%
  mutate(prop = Freq/sum(Freq))

ggplot(data = plotTable, mapping = aes(x = True, y = Prediction, fill = goodbad, alpha = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), vjust = .5, fontface  = "bold", alpha = 1) +
  scale_fill_manual(values = c(good = "green", bad = "red")) +
  theme_bw() +
  xlim(rev(levels(table$True))) +
  theme(axis.text.y = element_text(angle=90, hjust=0.5))


```
